{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gr64f Reinforcement based RL Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import scipy.stats as stats\n",
    "\n",
    "from flyjax.experiment.get import fetch_behavioral_data, get_experiments\n",
    "from flyjax.utils.plotting import plot_single_experiment_data\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style='ticks')\n",
    "sns.set_palette('colorblind')\n",
    "plt.rcParams['font.sans-serif'] = 'Arial'\n",
    "\n",
    "def significance_stars(p):\n",
    "    if p < 0.001:\n",
    "        return '***'\n",
    "    elif p < 0.01:\n",
    "        return '**'\n",
    "    elif p < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "data_folder = '../../../data/Gr64f/dmVS_14-09-2023/' # folder containing the data\n",
    "processed_folder = 'processed_data' # folder containing the processed data\n",
    "minimal_trials = 180 # minimal number of trials to consider a fly to be included in the analysis\n",
    "\n",
    "choices, rewards, metadata = fetch_behavioral_data(data_folder, minimal_trials, remove_control=True)\n",
    "\n",
    "n_experiments = len(metadata)\n",
    "\n",
    "# set the seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a literature inspired Model for Reinforcement Learning\n",
    "Based on Differential Forgetting Q-Learning (DFQ) by Ito and Doya (2009)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flyjax.fitting.cv import parallel_k_fold_cross_validation_train\n",
    "from flyjax.fitting.samplers import base_randn_sampler\n",
    "from flyjax.agent.rl.zoo import differential_forgetting_q_agent, differential_q_agent, forgetting_q_agent, q_agent\n",
    "from flyjax.agent.base import random_policy\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [(choices[i], rewards[i]) for i in range(n_experiments)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the DFQ Model to the Gr64f Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_param_sampler = partial(base_randn_sampler, n_params=4)\n",
    "\n",
    "total_pred_ll, per_experiment_ll, params = parallel_k_fold_cross_validation_train(\n",
    "    experiments=experiments,\n",
    "    k=5,\n",
    "    init_param_sampler=init_param_sampler,\n",
    "    agent=differential_forgetting_q_agent,\n",
    "    learning_rate=5e-2,\n",
    "    num_steps=10000,\n",
    "    n_restarts=10,\n",
    "    min_num_converged=3,\n",
    "    early_stopping={\"min_delta\": 1e-4}\n",
    ")\n",
    "\n",
    "# make a nested dictionary with the results\n",
    "results = {\n",
    "    'total_log_likelihood': total_pred_ll,\n",
    "    'subject_log_likelihood': per_experiment_ll,\n",
    "    'fold_params': params\n",
    "}\n",
    "# save the results as a pickle file\n",
    "results_file = os.path.join(processed_folder, 'dfq_results.pkl')\n",
    "with open(results_file, 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Study\n",
    "We will test the DFQ model on the Gr64f data and compare it:\n",
    "1. Differential Q-Learning (DQ) model : by ablation of the forgetting term\n",
    "2. Forgetting Q-Learning (FQ) model : by ablation of the differential reward term\n",
    "3. Q-Learning (Q) model : by ablation of both the forgetting and differential reward terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the DQ Model to the Gr64f Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_param_sampler = partial(base_randn_sampler, n_params=3)\n",
    "\n",
    "total_pred_ll, per_experiment_ll, params = parallel_k_fold_cross_validation_train(\n",
    "    experiments=experiments,\n",
    "    k=5,\n",
    "    init_param_sampler=init_param_sampler,\n",
    "    agent=differential_q_agent,\n",
    "    learning_rate=5e-2,\n",
    "    num_steps=10000,\n",
    "    n_restarts=10,\n",
    "    min_num_converged=3,\n",
    "    early_stopping={\"min_delta\": 1e-4}\n",
    ")\n",
    "\n",
    "# make a nested dictionary with the results\n",
    "results = {\n",
    "    'total_log_likelihood': total_pred_ll,\n",
    "    'subject_log_likelihood': per_experiment_ll,\n",
    "    'fold_params': params\n",
    "}\n",
    "# save the results as a pickle file\n",
    "results_file = os.path.join(processed_folder, 'dq_results.pkl')\n",
    "with open(results_file, 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the FQ Model to the Gr64f Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_param_sampler = partial(base_randn_sampler, n_params=3)\n",
    "\n",
    "total_pred_ll, per_experiment_ll, params = parallel_k_fold_cross_validation_train(\n",
    "    experiments=experiments,\n",
    "    k=5,\n",
    "    init_param_sampler=init_param_sampler,\n",
    "    agent=forgetting_q_agent,\n",
    "    learning_rate=5e-2,\n",
    "    num_steps=10000,\n",
    "    n_restarts=10,\n",
    "    min_num_converged=3,\n",
    "    early_stopping={\"min_delta\": 1e-4}\n",
    ")\n",
    "\n",
    "# make a nested dictionary with the results\n",
    "results = {\n",
    "    'total_log_likelihood': total_pred_ll,\n",
    "    'subject_log_likelihood': per_experiment_ll,\n",
    "    'fold_params': params\n",
    "}\n",
    "# save the results as a pickle file\n",
    "results_file = os.path.join(processed_folder, 'fq_results.pkl')\n",
    "with open(results_file, 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_param_sampler = partial(base_randn_sampler, n_params=2)\n",
    "\n",
    "total_pred_ll, per_experiment_ll, params = parallel_k_fold_cross_validation_train(\n",
    "    experiments=experiments,\n",
    "    k=5,\n",
    "    init_param_sampler=init_param_sampler,\n",
    "    agent=q_agent,\n",
    "    learning_rate=5e-2,\n",
    "    num_steps=10000,\n",
    "    n_restarts=10,\n",
    "    min_num_converged=3,\n",
    "    early_stopping={\"min_delta\": 1e-4}\n",
    ")\n",
    "\n",
    "# make a nested dictionary with the results\n",
    "results = {\n",
    "    'total_log_likelihood': total_pred_ll,\n",
    "    'subject_log_likelihood': per_experiment_ll,\n",
    "    'fold_params': params\n",
    "}\n",
    "# save the results as a pickle file\n",
    "results_file = os.path.join(processed_folder, 'q_results.pkl')\n",
    "with open(results_file, 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the random policy model on the Gr64f Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flyjax.fitting.evaluation import negative_log_likelihood_experiment\n",
    "\n",
    "total_pred_ll = 0.0\n",
    "per_experiment_ll = {}\n",
    "for exp_idx, (choices, rewards) in enumerate(experiments):\n",
    "    # Compute predicted log likelihood: negative NLL.\n",
    "    ll = -float(\n",
    "        negative_log_likelihood_experiment(None, random_policy, choices, rewards)\n",
    "    )\n",
    "    per_experiment_ll[exp_idx] = ll\n",
    "    total_pred_ll += ll\n",
    "print(f\"predictive log-likelihood: {total_pred_ll:.2f}\")\n",
    "\n",
    "# make a nested dictionary with the results\n",
    "results = {\n",
    "    'total_log_likelihood': total_pred_ll,\n",
    "    'subject_log_likelihood': per_experiment_ll,\n",
    "    'fold_params': None\n",
    "}\n",
    "# save the results as a pickle file\n",
    "results_file = os.path.join(processed_folder, 'random_results.pkl')\n",
    "with open(results_file, 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and compare the Gr64f data with the DFQ model + ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the results\n",
    "dfq_results_file = os.path.join(processed_folder, 'dfq_results.pkl')\n",
    "dq_results_file = os.path.join(processed_folder, 'dq_results.pkl')\n",
    "fq_results_file = os.path.join(processed_folder, 'fq_results.pkl')\n",
    "q_results_file = os.path.join(processed_folder, 'q_results.pkl')\n",
    "random_results_file = os.path.join(processed_folder, 'random_results.pkl')\n",
    "\n",
    "with open(dfq_results_file, 'rb') as f:\n",
    "    dfq_results = pickle.load(f)\n",
    "with open(dq_results_file, 'rb') as f:\n",
    "    dq_results = pickle.load(f)\n",
    "with open(fq_results_file, 'rb') as f:\n",
    "    fq_results = pickle.load(f)\n",
    "with open(q_results_file, 'rb') as f:\n",
    "    q_results = pickle.load(f)\n",
    "with open(random_results_file, 'rb') as f:\n",
    "    random_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are nested models, so we can compare them using a likelihood ratio test. We will additionally compare the models using the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) and also percentage of variance explained by the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the model to evaluate is DFQ, we will first use the log likelihood ratio test to compare it to the other models\n",
    "from flyjax.fitting.model_comparison import likelihood_ratio_test, aic, bic\n",
    "\n",
    "# compute the log likelihood ratio test of DFQ every other model\n",
    "dfq_vs_dq = likelihood_ratio_test(\n",
    "            ll_full=dfq_results['total_log_likelihood'],\n",
    "            ll_restricted=dq_results['total_log_likelihood'],\n",
    "            num_params_full=4,\n",
    "            num_params_restricted=3\n",
    "            )\n",
    "dfq_vs_fq = likelihood_ratio_test(\n",
    "            ll_full=dfq_results['total_log_likelihood'],\n",
    "            ll_restricted=fq_results['total_log_likelihood'],\n",
    "            num_params_full=4,\n",
    "            num_params_restricted=3\n",
    "            )\n",
    "dfq_vs_q = likelihood_ratio_test(\n",
    "            ll_full=dfq_results['total_log_likelihood'],\n",
    "            ll_restricted=q_results['total_log_likelihood'],\n",
    "            num_params_full=4,\n",
    "            num_params_restricted=2\n",
    "            )\n",
    "dfq_vs_random = likelihood_ratio_test(\n",
    "            ll_full=dfq_results['total_log_likelihood'],\n",
    "            ll_restricted=random_results['total_log_likelihood'],\n",
    "            num_params_full=4,\n",
    "            num_params_restricted=0\n",
    "            )\n",
    "print(f\"DFQ vs DQ: {dfq_vs_dq:.2f}; DFQ vs FQ: {dfq_vs_fq:.2f}; DFQ vs Q: {dfq_vs_q:.2f}; DFQ vs Random: {dfq_vs_random:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the likelihood ratio test is significant for the nested models suggests that every term in the DFQ model is important for explaining the data. Lets plot the results and compare the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subject-level dataframe\n",
    "dfq_subjects = pd.DataFrame(dfq_results['subject_log_likelihood'].items(), columns=['subject', 'log_likelihood'])\n",
    "dfq_subjects['model'] = 'DFQ'\n",
    "dq_subjects = pd.DataFrame(dq_results['subject_log_likelihood'].items(), columns=['subject', 'log_likelihood'])\n",
    "dq_subjects['model'] = 'DQ'\n",
    "fq_subjects = pd.DataFrame(fq_results['subject_log_likelihood'].items(), columns=['subject', 'log_likelihood'])\n",
    "fq_subjects['model'] = 'FQ'\n",
    "q_subjects = pd.DataFrame(q_results['subject_log_likelihood'].items(), columns=['subject', 'log_likelihood'])\n",
    "q_subjects['model'] = 'Q'\n",
    "random_subjects = pd.DataFrame(random_results['subject_log_likelihood'].items(), columns=['subject', 'log_likelihood'])\n",
    "random_subjects['model'] = 'Random'\n",
    "# concatenate the dataframes\n",
    "subjects = pd.concat([dfq_subjects, dq_subjects, fq_subjects, q_subjects, random_subjects])\n",
    "\n",
    "# sort by model and subject\n",
    "subjects = subjects.sort_values(by=['model', 'subject']).reset_index(drop=True)\n",
    "\n",
    "# get index for DFQ model\n",
    "dfq_idx = subjects[subjects['subject']==0].reset_index(drop=True)\n",
    "dfq_idx = dfq_idx[dfq_idx['model']=='DFQ'].index[0]\n",
    "print(dfq_idx)\n",
    "\n",
    "# for each subject, get the number of trials\n",
    "subjects['n_trials'] = subjects['subject'].apply(lambda x: len(experiments[x][0]))\n",
    "# get normalized likelihoods exp(log likelihood/n_trials)\n",
    "subjects['normalized_likelihood'] = np.exp(subjects['log_likelihood']/subjects['n_trials'])\n",
    "\n",
    "# from each subject, divide the log likelihood of the DFQ model\n",
    "subjects['normalized_likelihood_change'] = subjects.groupby('subject')['normalized_likelihood'].transform(lambda x: x/x.iloc[0])\n",
    "# convert the log likelihoods to a percentage scale\n",
    "subjects['normalized_likelihood_change'] = (subjects['normalized_likelihood_change']-1)*100\n",
    "\n",
    "\n",
    "# plot the log likelihoods of the different models with paired scatter plots\n",
    "plt.figure(figsize=(2, 4))\n",
    "# drop DFQ vs DFQ\n",
    "subjects_change = subjects[subjects['model'] != 'DFQ']\n",
    "order = subjects_change.groupby('model')['normalized_likelihood_change'].median().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "# change the names of the models\n",
    "subjects_change.loc[:, 'model'] = subjects_change['model'].replace({\n",
    "    'DQ': 'No Forgetting',\n",
    "    'FQ': 'No Choice Bias',\n",
    "    'Q': 'No Forgetting\\nor Choice Bias',\n",
    "    'Random': 'No Learning'\n",
    "})\n",
    "order_changed = ['No Learning' if x=='Random' else x for x in order]\n",
    "order_changed = ['No Forgetting\\nor Choice Bias' if x=='Q' else x for x in order_changed]\n",
    "order_changed = ['No Forgetting' if x=='DQ' else x for x in order_changed]\n",
    "order_changed = ['No Choice Bias' if x=='FQ' else x for x in order_changed]\n",
    "print(order_changed)\n",
    "\n",
    "\n",
    "# plot the change in log likelihoods of the different models with paired scatter plots\n",
    "plt.figure(figsize=(2, 4))\n",
    "sns.stripplot(\n",
    "    x='model', y='normalized_likelihood_change', data=subjects_change, \n",
    "    jitter=True, alpha=0.5, palette='colorblind', hue='model', size=2, \n",
    "    zorder=0, order=order_changed\n",
    ")\n",
    "# plot median with error bars = 95% confidence interval of the median\n",
    "sns.pointplot(\n",
    "    x='model', y='normalized_likelihood_change', data=subjects_change, \n",
    "    markers='_', capsize=0.2, color='black', zorder=1, \n",
    "    linewidth=1.5, linestyles='none', order=order_changed,\n",
    "    estimator=lambda x: np.median(x), errorbar=('ci', 95)\n",
    ")\n",
    "plt.ylabel('Difference in Normalized\\nLikelihood (% points)')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "sns.despine()\n",
    "plt.yscale('symlog')\n",
    "plt.ylim(-100, 10)\n",
    "plt.yticks([-100, -10, 0, 10], ['-100', '-10', '0', '10'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('')\n",
    "# add the significance stars for the paired Wilcoxon signed-rank test between DFQ and the other models\n",
    "for i, model in enumerate(order):\n",
    "    if model == 'DFQ':\n",
    "        continue\n",
    "    # perform the Wilcoxon signed-rank test\n",
    "    dfq_model = subjects[subjects['model'] == 'DFQ'].sort_values(by='subject')['normalized_likelihood'].values\n",
    "    other_model = subjects[subjects['model'] == model].sort_values(by='subject')['normalized_likelihood'].values\n",
    "    _, p = stats.wilcoxon(dfq_model, other_model)\n",
    "    # adjust for multiple comparisons (Bonferroni correction)\n",
    "    p *= len(subjects_change['model'].unique())\n",
    "    # add the significance stars\n",
    "    plt.text(i, 2, significance_stars(p), ha='center', va='center')\n",
    "    print(f\"DFQ vs {model}: p-val:{p:.2e},{significance_stars(p)}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the subjects most affected by the change in log likelihood for each model\n",
    "from flyjax.fitting.evaluation import get_state_and_probs\n",
    "import matplotlib.colors as colors      # for truncating colormaps\n",
    "\n",
    "# define function for truncating colormaps\n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "# define truncated colormap of RdYlGn for Orange-Green\n",
    "OrGn = truncate_colormap(plt.get_cmap('RdYlGn'), 0.3, 0.9)\n",
    "\n",
    "compare_models = order_changed.copy()\n",
    "compare_models.remove('No Learning')\n",
    "\n",
    "# get the top 5 subjects for each model\n",
    "for model in compare_models:\n",
    "    print(f\"Model: {model}\")\n",
    "    most_decreased = subjects_change[subjects_change['model'] == model].sort_values(by='normalized_likelihood_change').reset_index(drop=True).head(5)\n",
    "    # get the index of the subjects in the original dataframe\n",
    "    subjects_idx = most_decreased['subject'].values\n",
    "    # get the experiment data for these subjects\n",
    "    experiments_to_plot = [experiments[i] for i in subjects_idx]\n",
    "    # plot the data\n",
    "    for i in range(len(experiments_to_plot)):\n",
    "        # get the agent parameters (use the mean parameters across folds)\n",
    "        dfq_params = np.array([dfq_results['fold_params'][i] for i in dfq_results['fold_params'].keys()]).mean(axis=0)\n",
    "        if model == 'No Forgetting':\n",
    "            agent_params = np.array([dq_results['fold_params'][i] for i in dq_results['fold_params'].keys()]).mean(axis=0)\n",
    "            agent = differential_q_agent\n",
    "        elif model == 'No Choice Bias':\n",
    "            agent_params = np.array([fq_results['fold_params'][i] for i in fq_results['fold_params'].keys()]).mean(axis=0)\n",
    "            agent = forgetting_q_agent\n",
    "        elif model == 'No Forgetting\\nor Choice Bias':\n",
    "            agent_params = np.array([q_results['fold_params'][i] for i in q_results['fold_params'].keys()]).mean(axis=0)\n",
    "            agent = q_agent\n",
    "        else:\n",
    "            raise ValueError('Model not recognized')\n",
    "        # get the choices and rewards\n",
    "        choices, rewards = experiments_to_plot[i]\n",
    "        # get the state and probabilities for dfq\n",
    "        dfq_states, dfq_probs = get_state_and_probs(dfq_params, agent=differential_forgetting_q_agent, choices=choices, rewards=rewards)\n",
    "        # get the state and probabilities for the other model\n",
    "        states, probs = get_state_and_probs(agent_params, agent=agent, choices=choices, rewards=rewards)\n",
    "        # plot the data\n",
    "        n_trials = len(choices)\n",
    "        plt.figure(figsize=(5, 1))\n",
    "        c = choices\n",
    "        r = rewards\n",
    "        col = ((((r+1)*(1-2*c))+2)/4)\n",
    "        plt.scatter(np.arange(n_trials), c, c=col, cmap=OrGn, s=20,marker='|')\n",
    "        # plot the dfq probabilities for option 2\n",
    "        plt.plot(dfq_probs[:, 1], color='black')\n",
    "        # plot the probabilities for the other model\n",
    "        plt.plot(probs[:, 1], color='red')\n",
    "        clean_model_name = model.replace(\"\\n\", \" \")\n",
    "        plt.title(f'Subject {subjects_idx[i]} - {clean_model_name}')\n",
    "        plt.ylabel('Choice')\n",
    "        plt.box(False)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additive Model Comparison to see if other features are important\n",
    "\n",
    "We will also see if other features are important by comparing the DFQ model to more advanced models by adding more features to the model such as:\n",
    "1) Learnt initial Q values\n",
    "2) Epsilon softmax mixed policy\n",
    "3) Different learning rates for positive and negative reward prediction errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flyjax.agent.rl.zoo import dfq_agent_with_dual_lr, dfq_agent_with_epsilon_softmax, dfq_agent_with_init, dfq_agent_with_dynamic_exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_param_sampler = partial(base_randn_sampler, n_params=6)\n",
    "\n",
    "total_pred_ll, per_experiment_ll, params = parallel_k_fold_cross_validation_train(\n",
    "    experiments=experiments,\n",
    "    k=5,\n",
    "    init_param_sampler=init_param_sampler,\n",
    "    agent=dfq_agent_with_init,\n",
    "    learning_rate=5e-2,\n",
    "    num_steps=10000,\n",
    "    n_restarts=10,\n",
    "    min_num_converged=3,\n",
    "    early_stopping={\"min_delta\": 1e-4}\n",
    ")\n",
    "\n",
    "# make a nested dictionary with the results\n",
    "results = {\n",
    "    'total_log_likelihood': total_pred_ll,\n",
    "    'subject_log_likelihood': per_experiment_ll,\n",
    "    'fold_params': params\n",
    "}\n",
    "# save the results as a pickle file\n",
    "results_file = os.path.join(processed_folder, 'dfq_init_results.pkl')\n",
    "with open(results_file, 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_param_sampler = partial(base_randn_sampler, n_params=5)\n",
    "\n",
    "total_pred_ll, per_experiment_ll, params = parallel_k_fold_cross_validation_train(\n",
    "    experiments=experiments,\n",
    "    k=5,\n",
    "    init_param_sampler=init_param_sampler,\n",
    "    agent=dfq_agent_with_epsilon_softmax,\n",
    "    learning_rate=5e-2,\n",
    "    num_steps=10000,\n",
    "    n_restarts=10,\n",
    "    min_num_converged=3,\n",
    "    early_stopping={\"min_delta\": 1e-4}\n",
    ")\n",
    "\n",
    "# make a nested dictionary with the results\n",
    "results = {\n",
    "    'total_log_likelihood': total_pred_ll,\n",
    "    'subject_log_likelihood': per_experiment_ll,\n",
    "    'fold_params': params\n",
    "}\n",
    "# save the results as a pickle file\n",
    "results_file = os.path.join(processed_folder, 'dfq_epsilon_softmax_results.pkl')\n",
    "with open(results_file, 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_param_sampler = partial(base_randn_sampler, n_params=5)\n",
    "\n",
    "total_pred_ll, per_experiment_ll, params = parallel_k_fold_cross_validation_train(\n",
    "    experiments=experiments,\n",
    "    k=5,\n",
    "    init_param_sampler=init_param_sampler,\n",
    "    agent=dfq_agent_with_dual_lr,\n",
    "    learning_rate=5e-2,\n",
    "    num_steps=10000,\n",
    "    n_restarts=10,\n",
    "    min_num_converged=3,\n",
    "    early_stopping={\"min_delta\": 1e-4}\n",
    ")\n",
    "\n",
    "# make a nested dictionary with the results\n",
    "results = {\n",
    "    'total_log_likelihood': total_pred_ll,\n",
    "    'subject_log_likelihood': per_experiment_ll,\n",
    "    'fold_params': params\n",
    "}\n",
    "# save the results as a pickle file\n",
    "results_file = os.path.join(processed_folder, 'dfq_dual_lr_results.pkl')\n",
    "with open(results_file, 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_param_sampler = partial(base_randn_sampler, n_params=7)\n",
    "\n",
    "total_pred_ll, per_experiment_ll, params = parallel_k_fold_cross_validation_train(\n",
    "    experiments=experiments,\n",
    "    k=5,\n",
    "    init_param_sampler=init_param_sampler,\n",
    "    agent=dfq_agent_with_dynamic_exploration,\n",
    "    learning_rate=5e-2,\n",
    "    num_steps=10000,\n",
    "    n_restarts=10,\n",
    "    min_num_converged=3,\n",
    "    early_stopping={\"min_delta\": 1e-4}\n",
    ")\n",
    "\n",
    "# make a nested dictionary with the results\n",
    "results = {\n",
    "    'total_log_likelihood': total_pred_ll,\n",
    "    'subject_log_likelihood': per_experiment_ll,\n",
    "    'fold_params': params\n",
    "}\n",
    "# save the results as a pickle file\n",
    "results_file = os.path.join(processed_folder, 'dfq_dynamic_exploration_results.pkl')\n",
    "with open(results_file, 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the results\n",
    "dfq_init_results_file = os.path.join(processed_folder, 'dfq_init_results.pkl')\n",
    "dfq_epsilon_softmax_results_file = os.path.join(processed_folder, 'dfq_epsilon_softmax_results.pkl')\n",
    "dfq_dual_lr_results_file = os.path.join(processed_folder, 'dfq_dual_lr_results.pkl')\n",
    "dfq_dynamic_exploration_results_file = os.path.join(processed_folder, 'dfq_dynamic_exploration_results.pkl')\n",
    "\n",
    "with open(dfq_init_results_file, 'rb') as f:\n",
    "    dfq_init_results = pickle.load(f)\n",
    "with open(dfq_epsilon_softmax_results_file, 'rb') as f:\n",
    "    dfq_epsilon_softmax_results = pickle.load(f)\n",
    "with open(dfq_dual_lr_results_file, 'rb') as f:\n",
    "    dfq_dual_lr_results = pickle.load(f)\n",
    "with open(dfq_dynamic_exploration_results_file, 'rb') as f:\n",
    "    dfq_dynamic_exploration_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are still nested models, so we can compare them using a likelihood ratio test comparing the DFQ model to the more advanced models. We will additionally compare the models using the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the log likelihood ratio test between the different models\n",
    "from flyjax.fitting.model_comparison import likelihood_ratio_test, aic, bic\n",
    "\n",
    "dfq_vs_init = likelihood_ratio_test(\n",
    "            ll_full=dfq_init_results['total_log_likelihood'],\n",
    "            ll_restricted=dfq_results['total_log_likelihood'],\n",
    "            num_params_full=6,\n",
    "            num_params_restricted=4\n",
    "            )\n",
    "dfq_vs_epsilon_softmax = likelihood_ratio_test(\n",
    "            ll_full=dfq_epsilon_softmax_results['total_log_likelihood'],\n",
    "            ll_restricted=dfq_results['total_log_likelihood'],\n",
    "            num_params_full=5,\n",
    "            num_params_restricted=4\n",
    "            )\n",
    "dfq_vs_dual_lr = likelihood_ratio_test(\n",
    "            ll_full=dfq_dual_lr_results['total_log_likelihood'],\n",
    "            ll_restricted=dfq_results['total_log_likelihood'],\n",
    "            num_params_full=5,\n",
    "            num_params_restricted=4\n",
    "            )  \n",
    "dfq_vs_dynamic_exploration = likelihood_ratio_test(\n",
    "            ll_full=dfq_dynamic_exploration_results['total_log_likelihood'],\n",
    "            ll_restricted=dfq_results['total_log_likelihood'],\n",
    "            num_params_full=7,\n",
    "            num_params_restricted=4\n",
    "            )\n",
    "print(f\"DFQ vs Init: {dfq_vs_init:.2f}; DFQ vs Epsilon Softmax: {dfq_vs_epsilon_softmax:.2f}; DFQ vs Dual LR: {dfq_vs_dual_lr:.2f}; DFQ vs Dynamic Exploration: {dfq_vs_dynamic_exploration:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subject-level dataframe\n",
    "dfq_subjects = pd.DataFrame(dfq_results['subject_log_likelihood'].items(), columns=['subject', 'log_likelihood'])\n",
    "dfq_subjects['model'] = 'DFQ'\n",
    "dfq_init_subjects = pd.DataFrame(dfq_init_results['subject_log_likelihood'].items(), columns=['subject', 'log_likelihood'])\n",
    "dfq_init_subjects['model'] = 'DFQ with Init'\n",
    "dfq_epsilon_softmax_subjects = pd.DataFrame(dfq_epsilon_softmax_results['subject_log_likelihood'].items(), columns=['subject', 'log_likelihood'])\n",
    "dfq_epsilon_softmax_subjects['model'] = 'DFQ with Epsilon Softmax'\n",
    "dfq_dual_lr_subjects = pd.DataFrame(dfq_dual_lr_results['subject_log_likelihood'].items(), columns=['subject', 'log_likelihood'])\n",
    "dfq_dual_lr_subjects['model'] = 'DFQ with Dual LR'\n",
    "dfq_adv_subjects = pd.DataFrame(dfq_dynamic_exploration_results['subject_log_likelihood'].items(), columns=['subject', 'log_likelihood'])\n",
    "dfq_adv_subjects['model'] = 'DFQ with Dynamic Exploration'\n",
    "# concatenate the dataframes\n",
    "subjects = pd.concat([dfq_subjects, dfq_init_subjects, dfq_epsilon_softmax_subjects, dfq_dual_lr_subjects, dfq_adv_subjects])\n",
    "\n",
    "# sort by model and subject\n",
    "subjects = subjects.sort_values(by=['model', 'subject']).reset_index(drop=True)\n",
    "\n",
    "# get index for DFQ model\n",
    "dfq_idx = subjects[subjects['subject']==0].reset_index(drop=True)\n",
    "dfq_idx = dfq_idx[dfq_idx['model']=='DFQ'].index[0]\n",
    "print(dfq_idx)\n",
    "\n",
    "# for each subject, get the number of trials\n",
    "subjects['n_trials'] = subjects['subject'].apply(lambda x: len(experiments[x][0]))\n",
    "# get normalized likelihoods exp(log likelihood/n_trials)\n",
    "subjects['normalized_likelihood'] = np.exp(subjects['log_likelihood']/subjects['n_trials'])\n",
    "\n",
    "# from each subject, divide the log likelihood of the DFQ model\n",
    "subjects['normalized_likelihood_change'] = subjects.groupby('subject')['normalized_likelihood'].transform(lambda x: x/x.iloc[dfq_idx])\n",
    "# convert the log likelihoods to a percentage scale\n",
    "subjects['normalized_likelihood_change'] = (subjects['normalized_likelihood_change']-1)*100\n",
    "\n",
    "\n",
    "# drop DFQ vs DFQ\n",
    "subjects_change = subjects[subjects['model'] != 'DFQ']\n",
    "# get the order of the models by the average normalized likelihood change\n",
    "order = subjects_change.groupby('model')['normalized_likelihood_change'].median().sort_values(ascending=True).index.tolist()\n",
    "\n",
    "\n",
    "# change the names of the models\n",
    "subjects_change.loc[:, 'model'] = subjects_change['model'].replace({\n",
    "    'DFQ with Init': '+ Value Initialization',\n",
    "    'DFQ with Epsilon Softmax': '+ Fixed Exploration',\n",
    "    'DFQ with Dual LR': '+ Dual Learning Rates',\n",
    "    'DFQ with Dynamic Exploration': '+ Dynamic Exploration'\n",
    "})\n",
    "order_changed = [x.replace('DFQ with Init', '+ Value Initialization') for x in order]\n",
    "order_changed = [x.replace('DFQ with Epsilon Softmax', '+ Fixed Exploration') for x in order_changed]\n",
    "order_changed = [x.replace('DFQ with Dual LR', '+ Dual Learning Rates') for x in order_changed]\n",
    "order_changed = [x.replace('DFQ with Dynamic Exploration', '+ Dynamic Exploration') for x in order_changed]\n",
    "\n",
    "# plot the change in log likelihoods of the different models with paired scatter plots\n",
    "plt.figure(figsize=(2, 4))\n",
    "sns.stripplot(\n",
    "    x='model', y='normalized_likelihood_change', data=subjects_change, \n",
    "    jitter=True, alpha=0.5, palette='colorblind', hue='model', size=2, \n",
    "    zorder=0, order=order_changed\n",
    ")\n",
    "# plot median with error bars = 95% confidence interval of the median\n",
    "sns.pointplot(\n",
    "    x='model', y='normalized_likelihood_change', data=subjects_change, \n",
    "    markers='_', capsize=0.2, color='black', zorder=1, \n",
    "    linewidth=1.5, linestyles='none', order=order_changed,\n",
    "    estimator=lambda x: np.median(x), errorbar=('ci', 95)\n",
    ")\n",
    "plt.ylabel('Difference in Normalized\\nLikelihood (% points)')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "sns.despine()\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('')\n",
    "# add the significance stars for the paired Wilcoxon signed-rank test between DFQ and the other models\n",
    "for i, model in enumerate(order):\n",
    "    if model == 'DFQ':\n",
    "        continue\n",
    "    # perform the Wilcoxon signed-rank test\n",
    "    dfq_model = subjects[subjects['model'] == 'DFQ'].sort_values(by='subject')['normalized_likelihood'].values\n",
    "    other_model = subjects[subjects['model'] == model].sort_values(by='subject')['normalized_likelihood'].values\n",
    "    _, p = stats.wilcoxon(dfq_model, other_model)\n",
    "    # adjust for multiple comparisons (Bonferroni correction)\n",
    "    p *= len(subjects_change['model'].unique())\n",
    "    # add the significance stars\n",
    "    plt.text(i, 2, significance_stars(p), ha='center', va='center')\n",
    "    print(f\"DFQ vs {model}: p-val:{p:.2e},{significance_stars(p)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the subjects most affected by the change in log likelihood for each model\n",
    "from flyjax.fitting.evaluation import get_state_and_probs\n",
    "import matplotlib.colors as colors      # for truncating colormaps\n",
    "\n",
    "# define function for truncating colormaps\n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "# define truncated colormap of RdYlGn for Orange-Green\n",
    "OrGn = truncate_colormap(plt.get_cmap('RdYlGn'), 0.3, 0.9)\n",
    "\n",
    "compare_models = order_changed.copy()\n",
    "\n",
    "# get the top 5 subjects for each model\n",
    "for model in compare_models:\n",
    "    print(f\"Model: {model}\")\n",
    "    most_decreased = subjects_change[subjects_change['model'] == model].sort_values(by='normalized_likelihood_change').reset_index(drop=True).head(5)\n",
    "    # get the index of the subjects in the original dataframe\n",
    "    subjects_idx = most_decreased['subject'].values\n",
    "    # get the experiment data for these subjects\n",
    "    experiments_to_plot = [experiments[i] for i in subjects_idx]\n",
    "    # plot the data\n",
    "    for i in range(len(experiments_to_plot)):\n",
    "        # get the agent parameters (use the mean parameters across folds)\n",
    "        dfq_params = np.array([dfq_results['fold_params'][i] for i in dfq_results['fold_params'].keys()]).mean(axis=0)\n",
    "        if model == '+ Value Initialization':\n",
    "            agent_params = np.array([dfq_init_results['fold_params'][i] for i in dfq_init_results['fold_params'].keys()]).mean(axis=0)\n",
    "            agent = dfq_agent_with_init\n",
    "        elif model == '+ Fixed Exploration':\n",
    "            agent_params = np.array([dfq_epsilon_softmax_results['fold_params'][i] for i in dfq_epsilon_softmax_results['fold_params'].keys()]).mean(axis=0)\n",
    "            agent = dfq_agent_with_epsilon_softmax\n",
    "        elif model == '+ Dual Learning Rates':\n",
    "            agent_params = np.array([dfq_dual_lr_results['fold_params'][i] for i in dfq_dual_lr_results['fold_params'].keys()]).mean(axis=0)\n",
    "            agent = dfq_agent_with_dual_lr\n",
    "        elif model == '+ Dynamic Exploration':\n",
    "            agent_params = np.array([dfq_dynamic_exploration_results['fold_params'][i] for i in dfq_dynamic_exploration_results['fold_params'].keys()]).mean(axis=0)\n",
    "            agent = dfq_agent_with_dynamic_exploration\n",
    "        else:\n",
    "            raise ValueError('Model not recognized')\n",
    "        # get the choices and rewards\n",
    "        choices, rewards = experiments_to_plot[i]\n",
    "        # get the state and probabilities for dfq\n",
    "        dfq_states, dfq_probs = get_state_and_probs(dfq_params, agent=differential_forgetting_q_agent, choices=choices, rewards=rewards)\n",
    "        # get the state and probabilities for the other model\n",
    "        states, probs = get_state_and_probs(agent_params, agent=agent, choices=choices, rewards=rewards)\n",
    "        # plot the data\n",
    "        n_trials = len(choices)\n",
    "        plt.figure(figsize=(5, 1))\n",
    "        c = choices\n",
    "        r = rewards\n",
    "        col = ((((r+1)*(1-2*c))+2)/4)\n",
    "        plt.scatter(np.arange(n_trials), c, c=col, cmap=OrGn, s=20,marker='|')\n",
    "        # plot the dfq probabilities for option 2\n",
    "        plt.plot(dfq_probs[:, 1], color='black')\n",
    "        # plot the probabilities for the other model\n",
    "        plt.plot(probs[:, 1], color='red')\n",
    "        clean_model_name = model.replace(\"\\n\", \" \")\n",
    "        plt.title(f'Subject {subjects_idx[i]} - {clean_model_name}')\n",
    "        plt.ylabel('Choice')\n",
    "        plt.box(False)\n",
    "        plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
